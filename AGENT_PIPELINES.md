# CodeGeeX Agent Pipelines Documentation

Полная документация всех пайплайнов и схем работы агента CodeGeeX. Каждый пайплайн описан с детальными схемами потока данных, используемыми промтами и точками интеграции.

## Оглавление

1. [Пайплайн генерации кода (Code Generation Pipeline)](#1-пайплайн-генерации-кода)
2. [Пайплайн перевода кода (Code Translation Pipeline)](#2-пайплайн-перевода-кода)
3. [Пайплайн оценки бенчмарков (Benchmark Evaluation Pipeline)](#3-пайплайн-оценки-бенчмарков)
4. [Пайплайн веб-интерфейса (Web UI Pipeline)](#4-пайплайн-веб-интерфейса)
5. [Общая архитектура системы](#5-общая-архитектура-системы)

---

## 1. Пайплайн генерации кода

### Описание
Основной пайплайн для генерации кода на основе естественноязыкового описания или начального контекста кода. Используется в интерактивном режиме, веб-интерфейсе и бенчмарках.

### Точки входа

1. **Интерактивный режим / Тестирование**
   - **Файл:** `tests/test_inference.py`
   - **Функция:** `main()` (строка 118)
   - **Вызов:** `codegeex.generate()` (строка 167)

2. **Веб-интерфейс Gradio**
   - **Файл:** `deployment/server_gradio.py`
   - **Функция:** `predict()` (строка 108)
   - **Вызов:** `codegeex.generate()` (строка 121)

3. **Распределенная генерация для бенчмарков**
   - **Файл:** `codegeex/benchmark/humaneval-x/generate_humaneval_x.py`
   - **Функции:** `main()` (строка 202), `run_generation_distributed()` (строка 244)

### Схема потока данных

```
┌─────────────────────────────────────────────────────────────────────┐
│                         ПОЛЬЗОВАТЕЛЬСКИЙ ВВОД                        │
│                                                                      │
│  "Write a function to calculate factorial of n"                     │
│  или                                                                 │
│  "def factorial(n):\n    # Calculate factorial"                     │
└────────────────────────────┬────────────────────────────────────────┘
                             │
                             ▼
┌─────────────────────────────────────────────────────────────────────┐
│                    ШАГ 1: ОБРАБОТКА ПРОМТА                          │
│                    (Optional Language Tagging)                       │
│                                                                      │
│  Файл: codegeex/data/data_utils.py:7-64                            │
│  Функция: LANGUAGE_TAG[language]                                     │
│                                                                      │
│  Вход: prompt = "def factorial(n):"                                 │
│  Выход: "# language: Python\ndef factorial(n):"                     │
└────────────────────────────┬────────────────────────────────────────┘
                             │
                             ▼
┌─────────────────────────────────────────────────────────────────────┐
│                    ШАГ 2: ТОЧКА ВХОДА ГЕНЕРАЦИИ                     │
│                                                                      │
│  Файл: codegeex/__init__.py:15-69                                   │
│  Функция: generate()                                                 │
│                                                                      │
│  Параметры:                                                          │
│    - model: CodeGeeXModel                                            │
│    - tokenizer: CodeGeeXTokenizer                                    │
│    - prompt: str                                                     │
│    - out_seq_length: int (длина генерации)                          │
│    - seq_length: int (максимальная длина контекста, 2048)           │
│    - top_k: int (фильтрация top-k токенов)                          │
│    - top_p: float (nucleus sampling порог)                           │
│    - temperature: float (температура сэмплирования)                  │
│    - micro_batch_size: int (размер батча)                           │
└────────────────────────────┬────────────────────────────────────────┘
                             │
                             ▼
┌─────────────────────────────────────────────────────────────────────┐
│                    ШАГ 3: ТОКЕНИЗАЦИЯ                               │
│                                                                      │
│  Файл: codegeex/__init__.py:29                                      │
│  Метод: tokenizer.encode_code(prompt)                                │
│                                                                      │
│  Процесс:                                                            │
│    1. Разбиение текста на токены (GPT2-BPE алгоритм)                │
│    2. Преобразование токенов в ID                                    │
│    3. Сохранение длины промта: n_token_prompt                        │
│                                                                      │
│  Вход: "# language: Python\ndef factorial(n):"                      │
│  Выход: [3654, 3203, 25, 13814, 198, 4299, 1109, orial, 7, 77, ...] │
└────────────────────────────┬────────────────────────────────────────┘
                             │
                             ▼
┌─────────────────────────────────────────────────────────────────────┐
│                    ШАГ 4: СОЗДАНИЕ ПОТОКА ТОКЕНОВ                   │
│                                                                      │
│  Файл: codegeex/torch/inference.py:158-210                          │
│  Функция: get_token_stream()                                         │
│                                                                      │
│  Подготовка батча:                                                   │
│    pad_batch() - дополнение до seq_length токенами EOS              │
│    get_batch() - создание CUDA тензоров и attention masks           │
│                                                                      │
│  Создание attention mask (причинная маска):                          │
│    ┌─ 1  0  0  0  0 ─┐                                              │
│    │  1  1  0  0  0  │  Нижнетреугольная матрица                    │
│    │  1  1  1  0  0  │  1 = токен виден                             │
│    │  1  1  1  1  0  │  0 = токен замаскирован                      │
│    └─ 1  1  1  1  1 ─┘                                              │
│                                                                      │
│  Position IDs: [0, 1, 2, 3, 4, ..., seq_length-1]                   │
└────────────────────────────┬────────────────────────────────────────┘
                             │
                             ▼
┌─────────────────────────────────────────────────────────────────────┐
│                    ШАГ 5: АВТОРЕГРЕССИВНАЯ ГЕНЕРАЦИЯ                │
│                                                                      │
│  Файл: codegeex/torch/inference.py:217-327                          │
│  Функция: sample_sequence_batch()                                    │
│                                                                      │
│  ┌────────────────────────────────────────────────────────────┐    │
│  │  ЦИКЛ ГЕНЕРАЦИИ (context_length <= maxlen)                 │    │
│  │                                                              │    │
│  │  ┌─────────────────────────────────────────────────────┐   │    │
│  │  │  5.1. FORWARD PASS                                   │   │    │
│  │  │                                                       │   │    │
│  │  │  model(tokens, position_ids, attention_mask,         │   │    │
│  │  │        layer_past, get_key_value=True)               │   │    │
│  │  │                                                       │   │    │
│  │  │  Режимы:                                             │   │    │
│  │  │    - Recompute: обработка всей последовательности    │   │    │
│  │  │    - KV Cache: обработка только новых токенов        │   │    │
│  │  │                                                       │   │    │
│  │  │  Выход: logits [batch_size, vocab_size]              │   │    │
│  │  │          layer_past (KV cache для эффективности)     │   │    │
│  │  └─────────────────────────────────────────────────────┘   │    │
│  │                          ▼                                  │    │
│  │  ┌─────────────────────────────────────────────────────┐   │    │
│  │  │  5.2. ОБРАБОТКА ЛОГИТОВ                             │   │    │
│  │  │                                                       │   │    │
│  │  │  a) Фильтрация нежелательных ID (bad_ids)           │   │    │
│  │  │     logits[bad_ids] = -10000                         │   │    │
│  │  │                                                       │   │    │
│  │  │  b) Применение температуры                           │   │    │
│  │  │     logits = logits / temperature                    │   │    │
│  │  │                                                       │   │    │
│  │  │  c) Top-k фильтрация                                 │   │    │
│  │  │     - Сортировка логитов                             │   │    │
│  │  │     - Оставить только top-k                          │   │    │
│  │  │     - Остальные = -inf                               │   │    │
│  │  │                                                       │   │    │
│  │  │  d) Top-p (nucleus) фильтрация                       │   │    │
│  │  │     - Softmax для получения вероятностей             │   │    │
│  │  │     - Сумма вероятностей до достижения top_p        │   │    │
│  │  │     - Токены вне ядра = -inf                         │   │    │
│  │  └─────────────────────────────────────────────────────┘   │    │
│  │                          ▼                                  │    │
│  │  ┌─────────────────────────────────────────────────────┐   │    │
│  │  │  5.3. СЭМПЛИРОВАНИЕ ТОКЕНА                          │   │    │
│  │  │                                                       │   │    │
│  │  │  probs = softmax(logits)                             │   │    │
│  │  │  next_token = multinomial(probs, num_samples=1)      │   │    │
│  │  │                                                       │   │    │
│  │  │  Альтернатива (greedy):                              │   │    │
│  │  │  next_token = argmax(logits)                         │   │    │
│  │  └─────────────────────────────────────────────────────┘   │    │
│  │                          ▼                                  │    │
│  │  ┌─────────────────────────────────────────────────────┐   │    │
│  │  │  5.4. ОБНОВЛЕНИЕ ПОСЛЕДОВАТЕЛЬНОСТИ                 │   │    │
│  │  │                                                       │   │    │
│  │  │  tokens[:, context_length] = next_token              │   │    │
│  │  │  context_length += 1                                 │   │    │
│  │  └─────────────────────────────────────────────────────┘   │    │
│  │                          ▼                                  │    │
│  │  ┌─────────────────────────────────────────────────────┐   │    │
│  │  │  5.5. ПРОВЕРКА ЗАВЕРШЕНИЯ                           │   │    │
│  │  │                                                       │   │    │
│  │  │  if next_token == eos_token_id:                      │   │    │
│  │  │      is_done[batch_idx] = True                       │   │    │
│  │  │                                                       │   │    │
│  │  │  if all(is_done):                                    │   │    │
│  │  │      break  # Выход из цикла                         │   │    │
│  │  └─────────────────────────────────────────────────────┘   │    │
│  │                          ▼                                  │    │
│  │  Yield: (tokens, lengths) для текущего шага генерации      │    │
│  │                                                              │    │
│  └────────────────────────────────────────────────────────────┘    │
│                                                                      │
│  Выход: Generator[Tuple[Tensor, Tensor]]                            │
└────────────────────────────┬────────────────────────────────────────┘
                             │
                             ▼
┌─────────────────────────────────────────────────────────────────────┐
│                    ШАГ 6: ИЗВЛЕЧЕНИЕ КОДА                           │
│                                                                      │
│  Файл: codegeex/__init__.py:57-62                                   │
│                                                                      │
│  1. Получить финальные токены из генератора                         │
│  2. Удалить токены промта:                                           │
│     generated_tokens = all_tokens[n_token_prompt:]                  │
│  3. Декодировать в текст:                                            │
│     code = tokenizer.decode_code(generated_tokens)                  │
└────────────────────────────┬────────────────────────────────────────┘
                             │
                             ▼
┌─────────────────────────────────────────────────────────────────────┐
│                    ШАГ 7: ПОСТОБРАБОТКА КОДА                        │
│                                                                      │
│  Файл: codegeex/benchmark/utils.py:151-191                          │
│  Функция: cleanup_code()                                             │
│                                                                      │
│  Язык-специфичная очистка:                                           │
│                                                                      │
│  Python:                                                             │
│    - Обнаружение нового определения функции/класса                  │
│    - Проверка изменения уровня отступа                              │
│    - Удаление кода после "\ndef ", "\nclass ", "\nif __name__"     │
│                                                                      │
│  Java/C++/Go/JavaScript:                                             │
│    - Подсчет открывающих/закрывающих скобок                         │
│    - Обрезка на балансе скобок                                       │
│    - Удаление функций main() (если не часть решения)                │
│                                                                      │
│  Общие правила:                                                      │
│    - Удаление незавершенных строк                                    │
│    - Удаление комментариев после кода                                │
│    - Обрезка пробелов                                                │
└────────────────────────────┬────────────────────────────────────────┘
                             │
                             ▼
┌─────────────────────────────────────────────────────────────────────┐
│                         ВЫХОДНЫЕ ДАННЫЕ                              │
│                                                                      │
│  Формат зависит от контекста использования:                         │
│                                                                      │
│  Интерактивный режим / Web UI:                                       │
│    → Строка с сгенерированным кодом                                  │
│                                                                      │
│  Бенчмарк:                                                           │
│    {                                                                 │
│      "task_id": "Python/0",                                          │
│      "prompt": "<исходный промт>",                                   │
│      "generation": "<очищенный код>",                                │
│      "scores": -12.456,  // log probability                          │
│      "finish": 1,  // 0=incomplete, 1=early_stop, 2=eos             │
│      "output": [token_ids...]                                        │
│    }                                                                 │
└─────────────────────────────────────────────────────────────────────┘
```

### Детали реализации

#### Параметры сэмплирования

**Temperature (температура)**
- **Значение:** 0.0 - 1.0
- **Эффект:** Контролирует случайность генерации
  - `temperature = 0.2` → более детерминированный, консервативный код
  - `temperature = 0.8` → более креативный, разнообразный код
- **Формула:** `logits = logits / temperature`
- **Использование:** Низкая температура для production кода, высокая для исследования вариантов

**Top-k**
- **Значение:** 0 - 40 (обычно)
- **Эффект:** Оставляет только k токенов с наивысшими логитами
  - `top_k = 0` → отключено (все токены доступны)
  - `top_k = 40` → выбор только из 40 наиболее вероятных токенов
- **Применение:** Перед top-p фильтрацией

**Top-p (nucleus sampling)**
- **Значение:** 0.0 - 1.0
- **Эффект:** Динамический выбор минимального набора токенов с суммарной вероятностью >= p
  - `top_p = 0.95` → выбор из токенов, покрывающих 95% вероятностной массы
  - `top_p = 1.0` → все токены после top-k
- **Преимущество:** Адаптируется к распределению (больше токенов при равномерном распределении, меньше при острых пиках)

#### KV Cache оптимизация

**Без кэша (Recompute):**
```
Шаг 1: Forward([1, 2, 3, 4])       → token 5
Шаг 2: Forward([1, 2, 3, 4, 5])    → token 6
Шаг 3: Forward([1, 2, 3, 4, 5, 6]) → token 7
...
Сложность: O(n²) для генерации n токенов
```

**С KV Cache:**
```
Шаг 1: Forward([1, 2, 3, 4])       → token 5, cache_1
Шаг 2: Forward([5], cache_1)       → token 6, cache_2
Шаг 3: Forward([6], cache_2)       → token 7, cache_3
...
Сложность: O(n) для генерации n токенов
```

**Реализация:**
- `layer_past`: Кэш ключей и значений для каждого слоя внимания
- `get_key_value=True`: Возвращает обновленный кэш
- Размер кэша: `[batch, 2, num_heads, seq_len, head_dim]`

### Используемые промты

1. **Language Tags** (из PROMPTS_DOCUMENTATION.md, раздел 1)
   - Добавляются автоматически при указании языка
   - Пример: `"# language: Python\n"` + user_prompt

2. **Без явных system prompts**
   - Модель обучена на структуре кода как промте
   - Комментарии в коде служат инструкциями

### Точки интеграции

**Входные данные:**
- Пользовательский промт (строка)
- Опциональный язык программирования
- Параметры генерации (temperature, top_k, top_p, длина)

**Выходные данные:**
- Сгенерированный код (строка)
- Метаданные (для бенчмарков): scores, finish status, raw tokens

**Интеграция с другими пайплайнами:**
- → **Web UI Pipeline**: предоставляет генерацию по запросу
- → **Benchmark Evaluation Pipeline**: генерирует код для тестирования
- → **Code Translation Pipeline**: использует ту же базовую генерацию

### Оптимизации

1. **Квантизация модели** (8-bit)
   - Файл: `deployment/server_gradio.py:98-99`
   - Функция: `quantize(model, weight_bit_width=8)`
   - Эффект: Снижение использования памяти в 2 раза с минимальной потерей качества

2. **Distributed Generation**
   - ZeroMQ для распределения задач между GPU
   - Параллельная генерация множества сэмплов
   - Динамическая балансировка нагрузки

3. **Early Stopping**
   - Файл: `codegeex/benchmark/utils.py:115-149`
   - Функция: `is_code_generation_finished()`
   - Критерии остановки:
     - Обнаружена новая функция/класс
     - Изменился уровень отступа (Python)
     - Баланс скобок достигнут (C++/Java)

